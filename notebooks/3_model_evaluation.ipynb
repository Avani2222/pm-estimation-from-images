{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24ec30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data.dataset import Subset\n",
    "import os\n",
    "from PIL import Image\n",
    "import joblib\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f3695d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom PyTorch Dataset class.\n",
    "# It‚Äôs a data loader blueprint that tells PyTorch:\n",
    "# üëâ ‚ÄúHere‚Äôs how to find my data‚Äù\n",
    "# üëâ ‚ÄúHere‚Äôs how to read each image‚Äù\n",
    "# üëâ ‚ÄúHere‚Äôs how to get the labels‚Äù\n",
    "class AirQualityDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label_cols = ['AQI','PM2.5','PM10','O3','CO','SO2','NO2']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        filename = row[\"Filename\"].strip()  # remove leading/trailing spaces\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"File not found: {img_path}\")\n",
    "            return None  # optionally skip this sample\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Convert labels to float\n",
    "        labels = torch.tensor(\n",
    "            row[self.label_cols].astype(float).values,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        return img, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee8696d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_dataset = torch.load(\"/Users/avanigupta/pm-estimation-from-images/models/test_dataset.pt\", weights_only=False)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0518584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    if model_name == \"resnet18\":\n",
    "        model = models.resnet18(\n",
    "            weights=models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "        model.fc = nn.Linear(512, 7)\n",
    "\n",
    "    elif model_name == \"resnet34\":\n",
    "        model = models.resnet34(\n",
    "            weights=models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "        model.fc = nn.Linear(512, 7)\n",
    "\n",
    "    elif model_name == \"mobilenet_v2\":\n",
    "        model = models.mobilenet_v2(\n",
    "            weights=models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "        model.classifier[1] = nn.Linear(\n",
    "            model.classifier[1].in_features, 7\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model name\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "62b5cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(imgs)\n",
    "\n",
    "            preds_all.append(preds.cpu().numpy())\n",
    "            labels_all.append(labels.cpu().numpy())\n",
    "\n",
    "    preds_all = np.vstack(preds_all)\n",
    "    labels_all = np.vstack(labels_all)\n",
    "\n",
    "    return preds_all, labels_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15ba4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"RMSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"R2\": r2_score(y_true, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04d93c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mobilenet_v2 results:\n",
      "MSE: 0.0158\n",
      "RMSE: 0.0158\n",
      "MAE: 0.0779\n",
      "R2: 0.9685\n",
      "\n",
      "resnet18 results:\n",
      "MSE: 0.0176\n",
      "RMSE: 0.0176\n",
      "MAE: 0.0864\n",
      "R2: 0.9650\n",
      "\n",
      "resnet34 results:\n",
      "MSE: 0.0116\n",
      "RMSE: 0.0116\n",
      "MAE: 0.0610\n",
      "R2: 0.9768\n"
     ]
    }
   ],
   "source": [
    "model_names = [\n",
    "    \"mobilenet_v2\",\n",
    "    \"resnet18\",\n",
    "    \"resnet34\",\n",
    "\n",
    "]\n",
    "results = {}\n",
    "\n",
    "for name in model_names:\n",
    "    model = get_model(name).to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"/Users/avanigupta/pm-estimation-from-images/models/{name}_aqi.pth\", map_location=device)\n",
    "    )\n",
    "\n",
    "\n",
    "    y_pred, y_true = evaluate_model(model, test_loader)\n",
    "    metrics = compute_metrics(y_true, y_pred)\n",
    "\n",
    "    results[name] = metrics\n",
    "    print(f\"\\n{name} results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc7134d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AQI\n",
      "MAE: 0.07046648114919662\n",
      "\n",
      "PM2.5\n",
      "MAE: 0.05581687018275261\n",
      "\n",
      "PM10\n",
      "MAE: 0.0632089301943779\n",
      "\n",
      "O3\n",
      "MAE: 0.0635099709033966\n",
      "\n",
      "CO\n",
      "MAE: 0.051858607679605484\n",
      "\n",
      "SO2\n",
      "MAE: 0.061200570315122604\n",
      "\n",
      "NO2\n",
      "MAE: 0.06108921021223068\n"
     ]
    }
   ],
   "source": [
    "labels = ['AQI','PM2.5','PM10','O3','CO','SO2','NO2']\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"\\n{label}\")\n",
    "    print(\"MAE:\", mean_absolute_error(y_true[:, i], y_pred[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66f7c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.Subset'>\n",
      "(tensor([[[ 1.0502,  1.0502,  1.0502,  ...,  0.8104,  0.8104,  0.8104],\n",
      "         [ 1.0502,  1.0502,  1.0502,  ...,  0.8104,  0.8104,  0.8104],\n",
      "         [ 1.0502,  1.0502,  1.0502,  ...,  0.8104,  0.8104,  0.8104],\n",
      "         ...,\n",
      "         [-0.9192, -0.8849, -1.5014,  ...,  1.2043,  0.2111,  0.6563],\n",
      "         [-0.5082, -0.7822, -1.7583,  ...,  0.8961,  0.8104,  0.5193],\n",
      "         [-0.9020, -0.6452, -1.5357,  ...,  0.7248,  0.9303,  0.5878]],\n",
      "\n",
      "        [[ 1.3782,  1.3782,  1.3782,  ...,  1.1155,  1.1155,  1.1155],\n",
      "         [ 1.3782,  1.3782,  1.3782,  ...,  1.1155,  1.1155,  1.1155],\n",
      "         [ 1.3782,  1.3782,  1.3782,  ...,  1.1155,  1.1155,  1.1155],\n",
      "         ...,\n",
      "         [-1.0903, -1.0203, -1.6331,  ...,  0.6604, -0.3550,  0.1001],\n",
      "         [-0.6352, -0.9153, -1.8957,  ...,  0.3803,  0.2927, -0.0049],\n",
      "         [-1.0378, -0.7752, -1.6681,  ...,  0.2052,  0.4153,  0.0651]],\n",
      "\n",
      "        [[ 1.6291,  1.6291,  1.6291,  ...,  1.4548,  1.4548,  1.4548],\n",
      "         [ 1.6291,  1.6291,  1.6291,  ...,  1.4548,  1.4548,  1.4548],\n",
      "         [ 1.6291,  1.6291,  1.6291,  ...,  1.4548,  1.4548,  1.4548],\n",
      "         ...,\n",
      "         [-1.0898, -0.9853, -1.5604,  ...,  0.2696, -0.7413, -0.2881],\n",
      "         [-0.6367, -0.8807, -1.8044,  ..., -0.0441, -0.1312, -0.4275],\n",
      "         [-1.0376, -0.7761, -1.5953,  ..., -0.2184, -0.0092, -0.3578]]]), tensor([ 0.0301, -0.0304, -0.1657,  0.6901, -0.3068, -0.4250, -0.3039]))\n",
      "(tensor([[[-0.1486, -0.1486, -0.1486,  ...,  0.8789,  0.8789,  0.8789],\n",
      "         [-0.1486, -0.1486, -0.1486,  ...,  0.8789,  0.8789,  0.8789],\n",
      "         [-0.1486, -0.1486, -0.1486,  ...,  0.8789,  0.8789,  0.8789],\n",
      "         ...,\n",
      "         [-0.2171,  0.0912,  0.4337,  ...,  1.4612,  1.4954,  1.4612],\n",
      "         [-0.1999,  0.3138,  0.4679,  ...,  1.4440,  1.5297,  1.5468],\n",
      "         [-0.1828,  0.3994,  0.4679,  ...,  1.4783,  1.4783,  1.5125]],\n",
      "\n",
      "        [[ 0.5903,  0.5903,  0.5903,  ...,  1.5882,  1.5882,  1.5882],\n",
      "         [ 0.5903,  0.5903,  0.5903,  ...,  1.5882,  1.5882,  1.5882],\n",
      "         [ 0.5903,  0.5903,  0.5903,  ...,  1.5882,  1.5882,  1.5882],\n",
      "         ...,\n",
      "         [-0.0749,  0.2402,  0.5378,  ...,  1.7633,  1.7458,  1.6933],\n",
      "         [-0.0574,  0.4678,  0.5728,  ...,  1.7458,  1.7808,  1.7983],\n",
      "         [-0.0399,  0.5553,  0.5728,  ...,  1.7808,  1.7283,  1.7633]],\n",
      "\n",
      "        [[ 1.5420,  1.5420,  1.5420,  ...,  2.2391,  2.2391,  2.2391],\n",
      "         [ 1.5420,  1.5420,  1.5420,  ...,  2.2391,  2.2391,  2.2391],\n",
      "         [ 1.5420,  1.5420,  1.5420,  ...,  2.2391,  2.2391,  2.2391],\n",
      "         ...,\n",
      "         [ 0.1825,  0.4962,  0.7751,  ...,  2.0300,  2.0648,  2.0648],\n",
      "         [ 0.1999,  0.7228,  0.8099,  ...,  2.0125,  2.0997,  2.1171],\n",
      "         [ 0.2173,  0.8099,  0.8099,  ...,  2.0474,  2.0474,  2.0823]]]), tensor([-0.3534, -0.0679, -0.2015, -0.2143, -0.2619,  0.2857, -0.3036]))\n",
      "(tensor([[[ 0.2796,  0.2796,  0.2796,  ..., -0.0972, -0.1143, -0.1143],\n",
      "         [ 0.2796,  0.2796,  0.2796,  ..., -0.1143, -0.1143, -0.1314],\n",
      "         [ 0.2796,  0.2796,  0.2796,  ..., -0.1143, -0.1143, -0.1314],\n",
      "         ...,\n",
      "         [-1.4500, -1.7069, -1.2788,  ..., -0.7308, -1.2617, -1.0390],\n",
      "         [-1.4672, -1.5014, -1.0048,  ..., -1.3815, -1.1418, -1.3987],\n",
      "         [-1.3644, -1.0390, -0.9877,  ..., -1.2274, -1.4158, -1.4500]],\n",
      "\n",
      "        [[ 1.0805,  1.0805,  1.0805,  ...,  0.8004,  0.7829,  0.7829],\n",
      "         [ 1.0805,  1.0805,  1.0805,  ...,  0.7829,  0.7829,  0.7654],\n",
      "         [ 1.0805,  1.0805,  1.0805,  ...,  0.7829,  0.7829,  0.7654],\n",
      "         ...,\n",
      "         [-1.1253, -1.3880, -0.9503,  ..., -0.4951, -1.0378, -0.8102],\n",
      "         [-1.1429, -1.1779, -0.6352,  ..., -1.1779, -0.9678, -1.2304],\n",
      "         [-1.0378, -0.7052, -0.6176,  ..., -1.0203, -1.2479, -1.2829]],\n",
      "\n",
      "        [[ 1.9254,  1.9254,  1.9254,  ...,  1.6640,  1.6465,  1.6465],\n",
      "         [ 1.9254,  1.9254,  1.9254,  ...,  1.6465,  1.6465,  1.6291],\n",
      "         [ 1.9254,  1.9254,  1.9254,  ...,  1.6814,  1.6814,  1.6640],\n",
      "         ...,\n",
      "         [-1.2467, -1.5081, -1.0724,  ..., -0.8110, -1.3513, -1.1247],\n",
      "         [-1.2641, -1.2990, -0.7761,  ..., -1.4384, -1.2119, -1.4733],\n",
      "         [-1.1596, -0.8284, -0.7587,  ..., -1.2816, -1.4559, -1.4907]]]), tensor([-0.8722, -0.2346, -0.6343, -0.4929,  1.7500,  1.2143, -0.2321]))\n",
      "(tensor([[[ 1.7352,  1.7352,  1.7352,  ...,  1.3070,  1.3070,  1.3070],\n",
      "         [ 1.7352,  1.7352,  1.7352,  ...,  1.2899,  1.2899,  1.2899],\n",
      "         [ 1.7180,  1.7180,  1.7180,  ...,  1.2728,  1.2728,  1.2728],\n",
      "         ...,\n",
      "         [-0.4568, -0.3883, -0.5938,  ...,  0.6392,  0.6221,  0.0741],\n",
      "         [-0.5424, -0.5424, -0.6452,  ...,  1.8037,  1.8208,  0.6221],\n",
      "         [-0.6965, -0.4226, -0.4911,  ...,  1.7352,  1.3584,  0.4851]],\n",
      "\n",
      "        [[ 1.9559,  1.9559,  1.9559,  ...,  1.6933,  1.6933,  1.6933],\n",
      "         [ 1.9559,  1.9559,  1.9559,  ...,  1.6758,  1.6758,  1.6758],\n",
      "         [ 1.9384,  1.9384,  1.9384,  ...,  1.6583,  1.6583,  1.6583],\n",
      "         ...,\n",
      "         [-0.1099, -0.0399, -0.2500,  ...,  0.6604,  0.6078,  0.0826],\n",
      "         [-0.1275, -0.1275, -0.2675,  ...,  1.8158,  1.8158,  0.6254],\n",
      "         [-0.2500,  0.0301, -0.1099,  ...,  1.7283,  1.3431,  0.4503]],\n",
      "\n",
      "        [[ 2.2914,  2.2914,  2.2914,  ...,  2.0474,  2.0474,  2.0474],\n",
      "         [ 2.2914,  2.2914,  2.2914,  ...,  2.0300,  2.0300,  2.0300],\n",
      "         [ 2.2740,  2.2740,  2.2740,  ...,  2.0125,  2.0125,  2.0125],\n",
      "         ...,\n",
      "         [-0.1835, -0.1312, -0.3404,  ...,  0.7402,  0.7054,  0.1302],\n",
      "         [-0.2184, -0.2358, -0.3753,  ...,  1.8731,  1.8731,  0.6531],\n",
      "         [-0.3578, -0.0964, -0.2184,  ...,  1.7860,  1.3851,  0.4962]]]), tensor([ 0.0752,  0.2789,  0.5373, -0.3643,  0.1488,  0.0000, -0.3393]))\n",
      "(tensor([[[ 1.8722,  1.8722,  1.8379,  ...,  0.8618,  0.8447,  0.8447],\n",
      "         [ 1.8722,  1.8722,  1.8550,  ...,  0.8447,  0.8447,  0.8276],\n",
      "         [ 1.8893,  1.8722,  1.8550,  ...,  0.8276,  0.8276,  0.8104],\n",
      "         ...,\n",
      "         [ 0.5707,  0.5707,  0.5536,  ..., -0.5767, -0.6281, -0.0801],\n",
      "         [ 0.5364,  0.5364,  0.5364,  ..., -0.8507, -1.0048, -0.4568],\n",
      "         [ 0.5536,  0.5536,  0.5364,  ..., -0.3541, -0.1486, -0.3712]],\n",
      "\n",
      "        [[ 2.2710,  2.2710,  2.2710,  ...,  1.3081,  1.2906,  1.2906],\n",
      "         [ 2.2710,  2.2710,  2.2885,  ...,  1.2906,  1.2906,  1.2731],\n",
      "         [ 2.2885,  2.2710,  2.2885,  ...,  1.2906,  1.2906,  1.2731],\n",
      "         ...,\n",
      "         [ 1.0105,  1.0105,  1.0105,  ..., -0.2150, -0.2675,  0.2927],\n",
      "         [ 0.9755,  0.9755,  0.9930,  ..., -0.4951, -0.6527, -0.0924],\n",
      "         [ 0.9930,  0.9930,  0.9930,  ...,  0.0126,  0.2227, -0.0049]],\n",
      "\n",
      "        [[ 2.5877,  2.5877,  2.5703,  ...,  1.6988,  1.6814,  1.6814],\n",
      "         [ 2.5877,  2.5877,  2.5877,  ...,  1.6814,  1.6814,  1.6640],\n",
      "         [ 2.6051,  2.5877,  2.5877,  ...,  1.6814,  1.6814,  1.6640],\n",
      "         ...,\n",
      "         [ 1.4025,  1.4025,  1.4025,  ...,  0.1999,  0.1476,  0.7054],\n",
      "         [ 1.3677,  1.3677,  1.3851,  ..., -0.0790, -0.2358,  0.3219],\n",
      "         [ 1.3851,  1.3851,  1.3851,  ...,  0.4265,  0.6356,  0.4091]]]), tensor([ 1.3459,  0.9996,  0.5149, -0.6215, -0.2679,  1.5000,  0.0000]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "data = torch.load(\"/Users/avanigupta/pm-estimation-from-images/models/test_dataset.pt\", weights_only=False)\n",
    "\n",
    "# Check what type of object it is\n",
    "print(type(data))\n",
    "\n",
    "# If it's a list or dataset, iterate and print some examples\n",
    "for i, item in enumerate(data):\n",
    "    # Stop after 5 examples\n",
    "    if i >= 5:\n",
    "        break\n",
    "\n",
    "    # Try to print filename or metadata\n",
    "    if isinstance(item, dict):\n",
    "        # common pattern: {'image': tensor, 'filename': 'xxx.jpg', ...}\n",
    "        print(item.get(\"filename\", \"No filename key\"))\n",
    "    else:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c80e6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AQI': -0.7380762100219727, 'PM2.5': -0.2215963751077652, 'PM10': -0.43026039004325867, 'O3': -0.2799919843673706, 'CO': 0.7382581830024719, 'SO2': -0.3708515167236328, 'NO2': -0.18658989667892456}\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"resnet18\"  # choose the trained model\n",
    "model_path = f\"/Users/avanigupta/pm-estimation-from-images/models/{model_name}_aqi.pth\"\n",
    "img_path = \"/Users/avanigupta/pm-estimation-from-images/data/archive/Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/All_img/BENGR_Good_2023-02-19-08.30-1-1.jpg\"\n",
    "LABEL_COLS = ['AQI','PM2.5','PM10','O3','CO','SO2','NO2']\n",
    "# ----------------------------\n",
    "# Load model\n",
    "# ----------------------------\n",
    "model = get_model(model_name).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()  # important!\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocess image\n",
    "# ----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_tensor = transform(img).unsqueeze(0).to(device)  # add batch dimension\n",
    "\n",
    "# ----------------------------\n",
    "# Predict\n",
    "# ----------------------------\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "\n",
    "# Convert to list\n",
    "preds = output.squeeze(0).cpu().numpy()\n",
    "\n",
    "# Map to labels\n",
    "pred_dict = dict(zip(LABEL_COLS, [float(x) for x in preds]))\n",
    "\n",
    "print(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6476aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AQI': 55.045692443847656, 'PM2.5': 25.150537490844727, 'PM10': 56.691078186035156, 'O3': 20.021852493286133, 'CO': 174.64341735839844, 'SO2': 4.274399757385254, 'NO2': 13.922660827636719}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"resnet34\"\n",
    "model_path = f\"/Users/avanigupta/pm-estimation-from-images/models/{model_name}_aqi.pth\"\n",
    "scaler_path = \"/Users/avanigupta/pm-estimation-from-images/models/label_scaler.save\"\n",
    "\n",
    "LABEL_COLS = ['AQI','PM2.5','PM10','O3','CO','SO2','NO2']\n",
    "\n",
    "# ----------------------------\n",
    "# Load model\n",
    "# ----------------------------\n",
    "model = get_model(model_name).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Load scaler\n",
    "# ----------------------------\n",
    "scaler = joblib.load(\"/Users/avanigupta/pm-estimation-from-images/models/label_scaler.save\")\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocess image\n",
    "# ----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# Predict (scaled values)\n",
    "# ----------------------------\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "\n",
    "scaled_preds = output.cpu().numpy()  # shape (1, 7)\n",
    "\n",
    "# ----------------------------\n",
    "# Convert back to real values\n",
    "# ----------------------------\n",
    "real_preds = scaler.inverse_transform(scaled_preds)[0]\n",
    "\n",
    "pred_dict = {label: float(val) for label, val in zip(LABEL_COLS, real_preds)}\n",
    "print(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75e2da3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BENGR_Good_2023-02-19-08.30-1-1.jpg\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.basename(img_path)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09d73a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/avanigupta/pm-estimation-from-images/data/archive/Air Pollution Image Dataset/Air Pollution Image Dataset/Combined_Dataset/IND_and_Nep_AQI_Dataset.csv\")\n",
    "row = df[df[\"Filename\"] == filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4641e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQI    | Pred:    55.05 | Actual:    41.00\n",
      "PM2.5  | Pred:    25.15 | Actual:    20.00\n",
      "PM10   | Pred:    56.69 | Actual:    36.00\n",
      "O3     | Pred:    20.02 | Actual:     9.00\n",
      "CO     | Pred:   174.64 | Actual:   165.00\n",
      "SO2    | Pred:     4.27 | Actual:     3.00\n",
      "NO2    | Pred:    13.92 | Actual:     8.00\n"
     ]
    }
   ],
   "source": [
    "actual_values = row[LABEL_COLS].values[0]  # numpy array\n",
    "actual_dict = dict(zip(LABEL_COLS, actual_values))\n",
    "for label in LABEL_COLS:\n",
    "    print(f\"{label:6} | Pred: {pred_dict[label]:8.2f} | Actual: {actual_dict[label]:8.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQI range: 15 to 450\n",
    "# PM2.5 range: 4.0 to 500.0\n",
    "# PM10 range: 7.0 to 480.0\n",
    "# O3 range: 1.0 to 225.0\n",
    "# CO range: 0.0 to 410.0\n",
    "# SO2 range: 2.0 to 57.0\n",
    "# NO2 range: 0.67 to 169.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pm_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
